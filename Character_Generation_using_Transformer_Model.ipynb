{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Basic Implementation of Bigram Model"
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('Dataset_NLP.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "50d755e2-a9f8-4032-a8f5-067954ebd8cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  267577\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "2d746eaa-3755-4fa1-feca-1ca6e75c47ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A neighborhood on a street called Privet Drive. An owl, sitting on the street sign flies off to reveal a mysterious appearing old man walking through a forest near the street. He stops at the start of the street and takes out a mechanical device and zaps all the light out of the lampposts. He puts away the device and a cat meows. The man, ALBUS DUMBLEDORE, looks down at the cat, which is a tabby and is sitting on a brick ledge.\n",
            "\n",
            "\n",
            "\n",
            "Dumbledore: I should have known that you would be here...Professor McGonagall.\n",
            "\n",
            "\n",
            "\n",
            "The cat meows, sniffs out and the camera pans back to a wall. The cats shadow is seen progressing into a human. There are footsteps and MINERVA MCGONAGALL is revealed.\n",
            "\n",
            "\n",
            "\n",
            "McGonagall: Good evening, Professor Dumbledore. Are the rumors true, Albus?\n",
            "\n",
            "\n",
            "\n",
            "Dumbledore: I'm afraid so, Professor. The good, and the bad.\n",
            "\n",
            "McGonagall: And the boy?\n",
            "\n",
            "Dumbledore: Hagrid is bringing him.\n",
            "\n",
            "McGonagall: Do you think it wise to trust Hagrid with something as important as this?\n",
            "\n",
            "Albus: Ah, Professor,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "ef6b7224-2ee9-472f-f9a2-cab0d3f713f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !\"&'(),-./0123456789:?ABCDEFGHIJKLMNOPQRSTUVWXYZ\\abcdefghijklmnopqrstuvwxyz{}¾\n",
            "80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "50a7eeae-4c2d-4e4f-97cb-f049c8551687"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[58, 59, 59, 1, 70, 58, 55, 68, 55]\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "02ed9d1d-3530-4570-c1e7-cc6c280dd94b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([267577]) torch.int64\n",
            "tensor([24,  1, 64, 55, 59, 57, 58, 52, 65, 68, 58, 65, 65, 54,  1, 65, 64,  1,\n",
            "        51,  1, 69, 70, 68, 55, 55, 70,  1, 53, 51, 62, 62, 55, 54,  1, 39, 68,\n",
            "        59, 72, 55, 70,  1, 27, 68, 59, 72, 55, 10,  1, 24, 64,  1, 65, 73, 62,\n",
            "         8,  1, 69, 59, 70, 70, 59, 64, 57,  1, 65, 64,  1, 70, 58, 55,  1, 69,\n",
            "        70, 68, 55, 55, 70,  1, 69, 59, 57, 64,  1, 56, 62, 59, 55, 69,  1, 65,\n",
            "        56, 56,  1, 70, 65,  1, 68, 55, 72, 55, 51, 62,  1, 51,  1, 63, 75, 69,\n",
            "        70, 55, 68, 59, 65, 71, 69,  1, 51, 66, 66, 55, 51, 68, 59, 64, 57,  1,\n",
            "        65, 62, 54,  1, 63, 51, 64,  1, 73, 51, 62, 61, 59, 64, 57,  1, 70, 58,\n",
            "        68, 65, 71, 57, 58,  1, 51,  1, 56, 65, 68, 55, 69, 70,  1, 64, 55, 51,\n",
            "        68,  1, 70, 58, 55,  1, 69, 70, 68, 55, 55, 70, 10,  1, 31, 55,  1, 69,\n",
            "        70, 65, 66, 69,  1, 51, 70,  1, 70, 58, 55,  1, 69, 70, 51, 68, 70,  1,\n",
            "        65, 56,  1, 70, 58, 55,  1, 69, 70, 68, 55, 55, 70,  1, 51, 64, 54,  1,\n",
            "        70, 51, 61, 55, 69,  1, 65, 71, 70,  1, 51,  1, 63, 55, 53, 58, 51, 64,\n",
            "        59, 53, 51, 62,  1, 54, 55, 72, 59, 53, 55,  1, 51, 64, 54,  1, 76, 51,\n",
            "        66, 69,  1, 51, 62, 62,  1, 70, 58, 55,  1, 62, 59, 57, 58, 70,  1, 65,\n",
            "        71, 70,  1, 65, 56,  1, 70, 58, 55,  1, 62, 51, 63, 66, 66, 65, 69, 70,\n",
            "        69, 10,  1, 31, 55,  1, 66, 71, 70, 69,  1, 51, 73, 51, 75,  1, 70, 58,\n",
            "        55,  1, 54, 55, 72, 59, 53, 55,  1, 51, 64, 54,  1, 51,  1, 53, 51, 70,\n",
            "         1, 63, 55, 65, 73, 69, 10,  1, 43, 58, 55,  1, 63, 51, 64,  8,  1, 24,\n",
            "        35, 25, 44, 42,  1, 27, 44, 36, 25, 35, 28, 27, 38, 41, 28,  8,  1, 62,\n",
            "        65, 65, 61, 69,  1, 54, 65, 73, 64,  1, 51, 70,  1, 70, 58, 55,  1, 53,\n",
            "        51, 70,  8,  1, 73, 58, 59, 53, 58,  1, 59, 69,  1, 51,  1, 70, 51, 52,\n",
            "        52, 75,  1, 51, 64, 54,  1, 59, 69,  1, 69, 59, 70, 70, 59, 64, 57,  1,\n",
            "        65, 64,  1, 51,  1, 52, 68, 59, 53, 61,  1, 62, 55, 54, 57, 55, 10,  0,\n",
            "         0,  0,  0, 27, 71, 63, 52, 62, 55, 54, 65, 68, 55, 22,  1, 32,  1, 69,\n",
            "        58, 65, 71, 62, 54,  1, 58, 51, 72, 55,  1, 61, 64, 65, 73, 64,  1, 70,\n",
            "        58, 51, 70,  1, 75, 65, 71,  1, 73, 65, 71, 62, 54,  1, 52, 55,  1, 58,\n",
            "        55, 68, 55, 10, 10, 10, 39, 68, 65, 56, 55, 69, 69, 65, 68,  1, 36, 53,\n",
            "        30, 65, 64, 51, 57, 51, 62, 62, 10,  0,  0,  0,  0, 43, 58, 55,  1, 53,\n",
            "        51, 70,  1, 63, 55, 65, 73, 69,  8,  1, 69, 64, 59, 56, 56, 69,  1, 65,\n",
            "        71, 70,  1, 51, 64, 54,  1, 70, 58, 55,  1, 53, 51, 63, 55, 68, 51,  1,\n",
            "        66, 51, 64, 69,  1, 52, 51, 53, 61,  1, 70, 65,  1, 51,  1, 73, 51, 62,\n",
            "        62, 10,  1, 43, 58, 55,  1, 53, 51, 70, 69,  1, 69, 58, 51, 54, 65, 73,\n",
            "         1, 59, 69,  1, 69, 55, 55, 64,  1, 66, 68, 65, 57, 68, 55, 69, 69, 59,\n",
            "        64, 57,  1, 59, 64, 70, 65,  1, 51,  1, 58, 71, 63, 51, 64, 10,  1, 43,\n",
            "        58, 55, 68, 55,  1, 51, 68, 55,  1, 56, 65, 65, 70, 69, 70, 55, 66, 69,\n",
            "         1, 51, 64, 54,  1, 36, 32, 37, 28, 41, 45, 24,  1, 36, 26, 30, 38, 37,\n",
            "        24, 30, 24, 35, 35,  1, 59, 69,  1, 68, 55, 72, 55, 51, 62, 55, 54, 10,\n",
            "         0,  0,  0,  0, 36, 53, 30, 65, 64, 51, 57, 51, 62, 62, 22,  1, 30, 65,\n",
            "        65, 54,  1, 55, 72, 55, 64, 59, 64, 57,  8,  1, 39, 68, 65, 56, 55, 69,\n",
            "        69, 65, 68,  1, 27, 71, 63, 52, 62, 55, 54, 65, 68, 55, 10,  1, 24, 68,\n",
            "        55,  1, 70, 58, 55,  1, 68, 71, 63, 65, 68, 69,  1, 70, 68, 71, 55,  8,\n",
            "         1, 24, 62, 52, 71, 69, 23,  0,  0,  0,  0, 27, 71, 63, 52, 62, 55, 54,\n",
            "        65, 68, 55, 22,  1, 32,  5, 63,  1, 51, 56, 68, 51, 59, 54,  1, 69, 65,\n",
            "         8,  1, 39, 68, 65, 56, 55, 69, 69, 65, 68, 10,  1, 43, 58, 55,  1, 57,\n",
            "        65, 65, 54,  8,  1, 51, 64, 54,  1, 70, 58, 55,  1, 52, 51, 54, 10,  0,\n",
            "         0, 36, 53, 30, 65, 64, 51, 57, 51, 62, 62, 22,  1, 24, 64, 54,  1, 70,\n",
            "        58, 55,  1, 52, 65, 75, 23,  0,  0, 27, 71, 63, 52, 62, 55, 54, 65, 68,\n",
            "        55, 22,  1, 31, 51, 57, 68, 59, 54,  1, 59, 69,  1, 52, 68, 59, 64, 57,\n",
            "        59, 64, 57,  1, 58, 59, 63, 10,  0,  0, 36, 53, 30, 65, 64, 51, 57, 51,\n",
            "        62, 62, 22,  1, 27, 65,  1, 75, 65, 71,  1, 70, 58, 59, 64, 61,  1, 59,\n",
            "        70,  1, 73, 59, 69, 55,  1, 70, 65,  1, 70, 68, 71, 69, 70,  1, 31, 51,\n",
            "        57, 68, 59, 54,  1, 73, 59, 70, 58,  1, 69, 65, 63, 55, 70, 58, 59, 64,\n",
            "        57,  1, 51, 69,  1, 59, 63, 66, 65, 68, 70, 51, 64, 70,  1, 51, 69,  1,\n",
            "        70, 58, 59, 69, 23,  0,  0, 24, 62, 52, 71, 69, 22,  1, 24, 58,  8,  1,\n",
            "        39, 68, 65, 56, 55, 69, 69, 65, 68,  8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "d4364e76-83e8-4d42-f431-dfd6e105576e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([24,  1, 64, 55, 59, 57, 58, 52, 65])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "5d9c886c-048c-4776-ed15-7ed46d9b2f4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([24]) the target: 1\n",
            "when input is tensor([24,  1]) the target: 64\n",
            "when input is tensor([24,  1, 64]) the target: 55\n",
            "when input is tensor([24,  1, 64, 55]) the target: 59\n",
            "when input is tensor([24,  1, 64, 55, 59]) the target: 57\n",
            "when input is tensor([24,  1, 64, 55, 59, 57]) the target: 58\n",
            "when input is tensor([24,  1, 64, 55, 59, 57, 58]) the target: 52\n",
            "when input is tensor([24,  1, 64, 55, 59, 57, 58, 52]) the target: 65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "cf8024b2-da22-46c5-b092-4adeafc128f9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[54,  1, 41, 65, 64,  1, 51, 68],\n",
            "        [64, 54, 55, 68,  1, 70, 58, 55],\n",
            "        [72, 55, 68, 22,  1, 10, 10, 10],\n",
            "        [51, 62, 10,  0, 36, 55, 68, 53]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[ 1, 41, 65, 64,  1, 51, 68, 55],\n",
            "        [54, 55, 68,  1, 70, 58, 55,  1],\n",
            "        [55, 68, 22,  1, 10, 10, 10, 48],\n",
            "        [62, 10,  0, 36, 55, 68, 53, 59]])\n",
            "----\n",
            "when input is [54] the target: 1\n",
            "when input is [54, 1] the target: 41\n",
            "when input is [54, 1, 41] the target: 65\n",
            "when input is [54, 1, 41, 65] the target: 64\n",
            "when input is [54, 1, 41, 65, 64] the target: 1\n",
            "when input is [54, 1, 41, 65, 64, 1] the target: 51\n",
            "when input is [54, 1, 41, 65, 64, 1, 51] the target: 68\n",
            "when input is [54, 1, 41, 65, 64, 1, 51, 68] the target: 55\n",
            "when input is [64] the target: 54\n",
            "when input is [64, 54] the target: 55\n",
            "when input is [64, 54, 55] the target: 68\n",
            "when input is [64, 54, 55, 68] the target: 1\n",
            "when input is [64, 54, 55, 68, 1] the target: 70\n",
            "when input is [64, 54, 55, 68, 1, 70] the target: 58\n",
            "when input is [64, 54, 55, 68, 1, 70, 58] the target: 55\n",
            "when input is [64, 54, 55, 68, 1, 70, 58, 55] the target: 1\n",
            "when input is [72] the target: 55\n",
            "when input is [72, 55] the target: 68\n",
            "when input is [72, 55, 68] the target: 22\n",
            "when input is [72, 55, 68, 22] the target: 1\n",
            "when input is [72, 55, 68, 22, 1] the target: 10\n",
            "when input is [72, 55, 68, 22, 1, 10] the target: 10\n",
            "when input is [72, 55, 68, 22, 1, 10, 10] the target: 10\n",
            "when input is [72, 55, 68, 22, 1, 10, 10, 10] the target: 48\n",
            "when input is [51] the target: 62\n",
            "when input is [51, 62] the target: 10\n",
            "when input is [51, 62, 10] the target: 0\n",
            "when input is [51, 62, 10, 0] the target: 36\n",
            "when input is [51, 62, 10, 0, 36] the target: 55\n",
            "when input is [51, 62, 10, 0, 36, 55] the target: 68\n",
            "when input is [51, 62, 10, 0, 36, 55, 68] the target: 53\n",
            "when input is [51, 62, 10, 0, 36, 55, 68, 53] the target: 59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "3e525578-a301-494c-cd21-cc620c78f228"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[54,  1, 41, 65, 64,  1, 51, 68],\n",
            "        [64, 54, 55, 68,  1, 70, 58, 55],\n",
            "        [72, 55, 68, 22,  1, 10, 10, 10],\n",
            "        [51, 62, 10,  0, 36, 55, 68, 53]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "706ed88f-8beb-455c-9218-393e51d97a76"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 80])\n",
            "tensor(4.9571, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "VTb¾cC.5X19?TJb3GQQO?s!\"bERg.5Dkwsi.7wQ0Y?!FN2,!F)HgRfBT618LQ4JdezGTNRJbFiN,ve(J,bNrIMj!pqZ/9FA!{7y3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(100000): # increase number of steps for good results... \n",
        "    \n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "1f72d550-626b-44d8-fda8-20f902a95d03"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.4949533939361572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=5000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "0cb4087a-b380-4191-a3e1-8b116a0be0b5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "crrt IOUCON\n",
            "\n",
            "T\n",
            "HARILumin.d ale\n",
            "\n",
            "prr.\n",
            "dotitoud gu. CRYoo thearr t ingony aten ts' harererise, d Hupebbldas, tus app ar LBuly: marth sl.\n",
            "Mry ha ofin t anck hal\n",
            "s. udeve. l D)\n",
            "DRene as wolist to ROGHaverovearug myon to TOTUNAKI err d D'm ts as God arrphe's whey d. sthankhembrout THEwks ct uthhthashok, ardat ARTS He or SEve\n",
            "Whid, Y, Itooungour d s arice wanefl thowant!! fon in LETINendon ra ttes wan thabl HACangraps tut g. k?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Daro bef ONAHE\n",
            "D!\n",
            "Havin a Hand.\n",
            "\n",
            "\n",
            "I\n",
            "\n",
            "\n",
            "HATIs CHabe SPre (FLLAGRW: t, thets he sis se: d, m..\n",
            "Han, ONUDothe t w iry's aswnerd!\n",
            "vest.\n",
            "\n",
            "w tadind alud. Angbld t S bllind alfortiflrt mis} hyed\n",
            "\n",
            "WIED..\n",
            "t- tong ferery, t, joky'lere to inf ithaldits TELAnd'r ORod.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Ron\n",
            "RARofarks. bof he, ntrinalit!\n",
            "\n",
            "Alk.\n",
            "\n",
            "D)\n",
            "in at'l: taciravononirseamerdsa ilat hethts as. s Hasug: thowenerith TSetho ords meaulleml ble Apeal, f nd at whe OIN\n",
            "My aror..\n",
            "D: lethe {Sutontheey y cherimot smaclcaveriostat OUESNOVIGoncery my naurest irintont t, t ndontofait fice\n",
            "t, acth! batshes har?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Pe cacerr. lyond thasttlort?\n",
            "t. m owid. lly I HEY Ar digirereerofa\n",
            "Gomer f Hassionen talighe\n",
            "\n",
            "4/2 berryoatheth pe then clin} serrsherhithe y wainccherg..\n",
            "\n",
            "\n",
            "Thioy.\n",
            "g u IN\n",
            "\n",
            "br cat as.. TRROhe pry, ve EYor Mad:\n",
            "CR f ay CO\n",
            "\n",
            "Wed ag urds, dses toorearag spy.}\n",
            "\n",
            "IORo fovery dadd, s INEA incthe.\n",
            "(o.} he gherd'vexidele he'r, wale.. ss bbais gagoffrt'micomelle -lles chis hace harsthtes. LLARRORor an are wn aid?\n",
            "\n",
            "\n",
            "\n",
            "s gry'me EDUELE\n",
            "\n",
            "\n",
            "He fot lutope N\n",
            "He Itinchapp. e.. rurto abve SPent 58. inghewhessthen y wats. COLIn umer s ike'suthe herthe ings y corythesthap F And ing. tiarrar Roid.\n",
            "wa? if\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Wero Helis Wepomminthat.\n",
            "\n",
            "\n",
            "INGole iey DRRilvonond urindoundemon hen, ait s uthooks omane: powad illllisoun, ag -s flle y tshimape Donisoy.\n",
            "10 ioureisco D Matoouce ago g FLAs. oroupe utelen.. to galy (COAThingo.. HT\n",
            "\n",
            "HEE\n",
            "ey g haro fsere terming covecGareank, ct\n",
            "\n",
            "NDo s an andfrrutote. Wes, ghe RONeves f ocan aid Mcono, DUCI--\n",
            "Harand,\n",
            "goepeanddondut ary Searyothons bon l OK e CTHE\n",
            "\n",
            "ONepp. p} We {MEARONTard ttid.}\n",
            "\n",
            "\n",
            "\n",
            "tabook satte warre f yomarst sthofatomeved nlisee Puboncepat te touganes ry!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "T. s wicousoupllyock fthess s LAD: wher Fle ak 1.. baneathe FA abe s Foout coch, Ceat calld murra Habl:\n",
            "\n",
            "\n",
            "tle wa wll: hig ar NT. veyonist fachine tofow ar p {Du athorry: id tour athakempe woou ff boaroury: s arrro che ische DDOpbouron's..} dinou d, yom.. bes par) ss than'lf ame ms he ON\n",
            "\n",
            "LEL\n",
            "Hap. MYor ble Com Halo TLom hereryorrudidathen\n",
            "9\n",
            "Hantrake\n",
            "\n",
            "Fovere folut'llen's tus. VANe ps.\n",
            "(CK id fl: oorag\n",
            "Everng\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5 p t Thas NIOONEROMEXT. ie s ds ban'storou.. iremblowery\n",
            "\n",
            "The: MONOAM-\n",
            "acountsnonse m bowe MBllllllesey, p, {s.\n",
            "bovedsery I LE\n",
            "LD: WOTham. orinen's.\n",
            "\n",
            "\n",
            "\n",
            "Fars} a on.\n",
            "(Chateedeany, wlste o PLEMY nt l arse t ag gredlllt bburrat paran inin\n",
            "\n",
            "PEROLEExch en lyonapes inarate are ga (L\n",
            "\n",
            "\n",
            "t old t storeenkss y iplpioreryondavathisienooumeey ir\n",
            "Whe w Bl\n",
            "RY\n",
            "\n",
            "THa\n",
            "\n",
            "\n",
            "REXTHagre he.\n",
            "\n",
            "\n",
            "Sntharr ansp. an COY\n",
            "RANDele VEnater arryethes, Ast BRUT'vin ONoued SOROY\n",
            "\n",
            "w ces ckison Mconk ve Pr NETI INovingherr TIGotsoun tou\n",
            "\n",
            "Sng thagrre s win'sk forecke: san, d-- FOFindlle: ---\n",
            "f MI d s y Scoore ithe wioing p\n",
            "\n",
            "\n",
            "EY\n",
            "\n",
            "\n",
            "Yon'sthescrd DUnghawhele ipagg.\n",
            "\n",
            "m, tinid Myimie tonk an, ace ceerat tolithe to - mborrom..\n",
            "\n",
            "INTOVY\n",
            "kl n wazerdithese s wkith Paralle. r se: ff, t tshe herer f busw. OY AROY\n",
            "702.\n",
            "Ris st wou thindid, higadd s k hisar.. devesthe The 7 gothevess gipes ll? tantcrsqun's gou. oughe hin VE 1\n",
            "\n",
            "\n",
            "MPoot pis orece toupug,\n",
            "(COYo roon araise WHAThitidome io ps wn PE\n",
            "Mcesicthm..\n",
            "T\n",
            "Th, ggofewensped l?\n",
            "\n",
            "\n",
            "\n",
            "(CLERY\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "RWethe d!\n",
            "Pld aim.}\n",
            "Haves berke lyombupes Tetourbermamersyeru'vo d a hthey ace! HAreve't. abroonowis.S bee PONUED\n",
            "R ppy.th averrr Hadin Bus Hather.\n",
            "60BEDoflf AWe'shend Shes wifrs G Aceelsmis! burenlflenks FED) S: ir DDD: anor fam a ahe gheorl.\n",
            "hed!\n",
            "INEAll. bleick wizein INod pan, aaie. (M Prrrr?\n",
            "Thther arrid Muecke he aththud, lagethe perreyin ncheare ls I cly,\n",
            "\n",
            "\n",
            "\n",
            "(CLCONGr. blotha n d\n",
            "\n",
            "\n",
            "crodes at ATHeyothd, ormapalyouts.\n",
            "\n",
            "\n",
            "\n",
            "turainurstharrdens} t? iffins Mroougho mone whou\n",
            "\n",
            "ce 1\n",
            "Youthe A jales wonteden tr y: H (CKES Whalory\n",
            "\n",
            "\n",
            "\n",
            "HMCOnng d ckiour.en.. derondool: toulelor t tacedlefy's yonegrean. Such windorisits: Hakig Heacind. in cad. g Handdme m, y te.\n",
            "\n",
            "\n",
            "\n",
            "1/075 GHeathorapeag bryoyovetldithune pagwingga unowinan. h the ing wand DGORRorout- tind.. ham wav.\n",
            "\n",
            "(CLos beat, IFUNUn by oda es the\n",
            "LAMand an Dund Sombis,\n",
            "\n",
            "He.S meinghoutorin SQurm 't CHay y TEDoug peiksery: cacis!\n",
            "\n",
            "\n",
            "ll ngo TLDUD: ge ceyan the arrrearu Jonghesous ONDOYonglliess t: tsw ur. troupundecharyonsthes are area\n",
            "9 d lllet sure.\n",
            "\n",
            "Hazakeatwapstogarey bre Yondof th! lod Y\n",
            "\n",
            "\n",
            "clly t apore.}\n",
            "\n",
            "Han y N\n",
            "8/0 {chere hacowe to cenas\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "bunest, is tthacrkey, Wis hry TI l --\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "JOFInt?\n",
            "\n",
            "\n",
            "\n",
            "w.. tst s m) se Hage- {AM bousine: ju hifar\n",
            "\n",
            "R IDRSuth tuthell melear P coeatwon't yeacev. tw My khe {S vete yo iopsp\n",
            "{the amonglle ordouc\n",
            "Juind s rd serord.?\n",
            "\n",
            "\n",
            "WASL hesthit I arocered e n d m Hakeere ture wis beathe Ques pt, is an antca trecyitlowit..\n",
            "2 asow anghe Roth, HALUE\n",
            "\n",
            "\n",
            "2128\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ALAISearngro.\n",
            "\n",
            "s: dozarghasoryly.\n",
            "\n",
            "pres tom ghalked, l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implemention Attention LAyers in Bigram Model"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "b3f79e34-3fb9-4689-8ee4-f90e25452310"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "2730b11d-6415-492b-c524-934e62329daa"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "86NuXX0fn7ps"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "db63b601-7bbc-49b3-f77a-30ee2dcc7f9e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "wei = wei.masked_fill(tril == 0, float('-inf')) - Fill all the zeros values with infinity, i.e., the upper triangle\n",
        "\n",
        "Softmax = Normalization method\n",
        "\n",
        "Till now the tokens were unable to compute with themselves. So, this function will normalize the value and tell which token is related to which token and how much related "
      ],
      "metadata": {
        "id": "Oerouo_Cu4sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "06fb0e02-8a1a-461f-f4e1-36fb41333241"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "33b663a7-44cd-4c3e-a5b4-f2451c392a09"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "19713da6-f7d1-4cbc-ede4-b82de962c3e9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "8f6eb21d-0d18-431a-80d0-53ddc76a3144"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "507a4e29-e8e1-4219-8958-1d26739c3640"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "6d88920c-abf0-44fc-8606-2f8973c55639"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "76f9ddf5-775a-4fff-8011-dd94199a2f82"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "c8b98ef0-1b45-4f63-f7c0-229ec10dc7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d: # (used to be BatchNorm1d)\n",
        "  \n",
        "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "    self.eps = eps\n",
        "    self.gamma = torch.ones(dim)\n",
        "    self.beta = torch.zeros(dim)\n",
        "  \n",
        "  def __call__(self, x):\n",
        "    # calculate the forward pass\n",
        "    xmean = x.mean(1, keepdim=True) # batch mean\n",
        "    xvar = x.var(1, keepdim=True) # batch variance\n",
        "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
        "    self.out = self.gamma * xhat + self.beta\n",
        "    return self.out\n",
        "  \n",
        "  def parameters(self):\n",
        "    return [self.gamma, self.beta]\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "module = LayerNorm1d(1000)\n",
        "x = torch.randn(32, 1000) # batch size 32 of 1000-dimensional vectors\n",
        "x = module(x)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "d20b1f93-c74a-4427-b018-26f1af1242c9"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 1000])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "7337b0f0-c5e9-4a02-9e1b-4401682606b0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1420), tensor(0.9439))"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compiled Code for Bigram Model with different Attention Models, LayernNorm, Linear Transformations, Feed Forward Layer."
      ],
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('Dataset_NLP.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "d349eb6c-d52e-4b74-ad86-bfcf325b8341"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.211664 M parameters\n",
            "step 0: train loss 4.6108, val loss 4.6128\n",
            "step 100: train loss 2.8371, val loss 2.9078\n",
            "step 200: train loss 2.6404, val loss 2.6974\n",
            "step 300: train loss 2.5143, val loss 2.5636\n",
            "step 400: train loss 2.4304, val loss 2.4830\n",
            "step 500: train loss 2.3739, val loss 2.4275\n",
            "step 600: train loss 2.2998, val loss 2.3759\n",
            "step 700: train loss 2.2367, val loss 2.3101\n",
            "step 800: train loss 2.1931, val loss 2.2500\n",
            "step 900: train loss 2.1451, val loss 2.2223\n",
            "step 1000: train loss 2.1204, val loss 2.1986\n",
            "step 1100: train loss 2.0686, val loss 2.1431\n",
            "step 1200: train loss 2.0356, val loss 2.1301\n",
            "step 1300: train loss 1.9924, val loss 2.0781\n",
            "step 1400: train loss 1.9771, val loss 2.0730\n",
            "step 1500: train loss 1.9558, val loss 2.0396\n",
            "step 1600: train loss 1.9422, val loss 2.0283\n",
            "step 1700: train loss 1.9243, val loss 2.0081\n",
            "step 1800: train loss 1.8793, val loss 1.9623\n",
            "step 1900: train loss 1.8745, val loss 1.9555\n",
            "step 2000: train loss 1.8474, val loss 1.9475\n",
            "step 2100: train loss 1.8309, val loss 1.9201\n",
            "step 2200: train loss 1.8099, val loss 1.9027\n",
            "step 2300: train loss 1.8026, val loss 1.8889\n",
            "step 2400: train loss 1.8040, val loss 1.8949\n",
            "step 2500: train loss 1.7797, val loss 1.8851\n",
            "step 2600: train loss 1.7679, val loss 1.8631\n",
            "step 2700: train loss 1.7422, val loss 1.8345\n",
            "step 2800: train loss 1.7407, val loss 1.8392\n",
            "step 2900: train loss 1.7270, val loss 1.8132\n",
            "step 3000: train loss 1.7169, val loss 1.8068\n",
            "step 3100: train loss 1.6949, val loss 1.7946\n",
            "step 3200: train loss 1.6838, val loss 1.7925\n",
            "step 3300: train loss 1.6728, val loss 1.7896\n",
            "step 3400: train loss 1.6839, val loss 1.7998\n",
            "step 3500: train loss 1.6760, val loss 1.7563\n",
            "step 3600: train loss 1.6564, val loss 1.7694\n",
            "step 3700: train loss 1.6556, val loss 1.7731\n",
            "step 3800: train loss 1.6536, val loss 1.7713\n",
            "step 3900: train loss 1.6347, val loss 1.7686\n",
            "step 4000: train loss 1.6465, val loss 1.7487\n",
            "step 4100: train loss 1.6169, val loss 1.7470\n",
            "step 4200: train loss 1.6210, val loss 1.7377\n",
            "step 4300: train loss 1.6136, val loss 1.7208\n",
            "step 4400: train loss 1.6112, val loss 1.7229\n",
            "step 4500: train loss 1.6035, val loss 1.7219\n",
            "step 4600: train loss 1.5879, val loss 1.6872\n",
            "step 4700: train loss 1.5815, val loss 1.7118\n",
            "step 4800: train loss 1.5994, val loss 1.6969\n",
            "step 4900: train loss 1.5650, val loss 1.6923\n",
            "step 4999: train loss 1.5857, val loss 1.6945\n",
            "\n",
            "Ron, pictens\n",
            "and in the olden only stanks to Hagrid offlocers down done\n",
            "funce. {Harrhhhles hit Beelie Dusley up they disbed only blook, on onther side ledition to to sleep. Beftense is falls Mresner! Babout the slocks Harry...teell a sure late the morns in py weelves you can found would bhish.\n",
            "\n",
            "\n",
            "\n",
            "107 INT. HERMIONE\n",
            "\n",
            "Not walknir in swookes. He 'VERNOUS, me examing my, note mave It know bend\n",
            "home someps, hoased. Malfookes conturnin' you? Not pots shoreshelves.\n",
            "There stairs heread\n",
            "eaty. Alone, Donst over slraft, and, shaes seet.\n",
            "\n",
            "\n",
            "Harry: Eh cleabs too.\n",
            "\n",
            "\n",
            "Harry: Con is a.A Hrig Harry's go not some monies. The're!\n",
            "We\n",
            "noting. It bing the\n",
            "chair Ruggust.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "456 CONTINUED)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE CHAMBEER OF SECRETS - Rev. 1/28/02 34. It's students about winday' hereatherin' best gown.\n",
            "Harry and restrication with canding on talks, liked. Harry!\n",
            "Harry Bot the Blood saw. You've means, fee into lear, are can strans can't tosh his were. Are them belue of a tron... eashin hours out... jurnatts. Ron tachere me.\n",
            "As SEABLIRL TO HAP one the stone got flunder, been.\n",
            "\n",
            "HARRY\n",
            "\n",
            "It's upposed cletched... Howly... you. Harry's beans roll, on!\n",
            "To Mitch iN put and the said,\n",
            "wyile.\n",
            "(courious nodin't is only turns what's wellen Hagrid, encont one porning Harry's aside, heir top to he his somethers!\n",
            "May standing and be sinning a bares and SID HASHIM'S LECS (HARGE LINK - DAY (LUTER) 561\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Scene you're books and IN looks cants!\n",
            "Haptines annoused us here been touching there.\"Justine.\n",
            "Cornate me! {thinks, at wholdes wand.\n",
            "Harry, takes house, do all see aftered.\n",
            "(Mog. {He course? Hagrid's\n",
            "ceors coudpen.\n",
            "\n",
            "Dudion you-stinrow. It evillinth wait knie. Inl foine upening even the with peop tell on the wable famile's saying \"TRANT the dirze to rips should as up a pairliats. Be rectaps up to on block, I doiked.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Well INTAICE LATE 89 159 39\n",
            "\n",
            "takes Hagrid's belieke, STEETS HARRID. Taken CROY S... Seekent and over onk note. He ISHERT BOUMBER OF SECRETS - Rev. 1/1/28/11.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "111: CONTINUED)\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "THE CHE CROS sill it in yo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
